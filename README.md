La regresi칩n lineal es una t칠cnica clave en an치lisis de datos y machine learning, que nos ayuda a ajustar una recta a un conjunto de puntos, y tambi칠n podemos resolverla usando 치lgebra lineal.

En lugar de resolver la ecuaci칩n ![image](https://github.com/user-attachments/assets/89db3c3e-3704-4279-a86a-f7a76026e33c)
y=mx+b usando sumatorias, podemos usar matrices y vectores para hacerlo mucho m치s eficiente, especialmente cuando trabajamos con grandes vol칰menes de datos.

**F칩rmula cl치sica vs. Matricial**:
La f칩rmula cl치sica de regresi칩n lineal simple es: ![image](https://github.com/user-attachments/assets/b21a1896-82ba-4443-8518-45f976e3cf06)
donde m es la pendiente y b el intercepto.

Pero si representamos todo con matrices, la f칩rmula se convierte en: ![image](https://github.com/user-attachments/assets/1718295e-5369-46c6-8154-5defa27e8e89)
Donde X es la matriz de datos, y 洧띻 es el vector que contiene los coeficientes 洧녪 y 洧녴.

**쯇or qu칠 esto importa?**
Usar 치lgebra lineal permite resolver la regresi칩n lineal de manera m치s eficiente, especialmente cuando hay muchas variables.

Las bibliotecas como NumPy utilizan estas f칩rmulas matriciales para hacer los c치lculos de forma r치pida y optimizada.

As칤 que, aunque la f칩rmula cl치sica es f치cil de entender, el enfoque matricial es esencial para escalar el an치lisis de datos y es el que usan las herramientas modernas.
